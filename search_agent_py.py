# -*- coding: utf-8 -*-
"""search_agent.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TneN_4PEzNnI_Y0fEm9xZnGqNY7xjMsN
"""

import os
os.environ["TAVILY_API_KEY"] = "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"
os.environ["GROQ_API_KEY"] = "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"


from langchain.chat_models import init_chat_model
llm = init_chat_model("llama-3.1-8b-instant", model_provider="groq", temperature=0)

# Tool
from langchain_community.tools.tavily_search import TavilySearchResults
search_tool = TavilySearchResults(max_results=5)


from langchain.agents import create_agent


prompt = "you are expert of history and searching the information"
agent = create_agent(
    llm,
    tools=[search_tool],
    system_prompt=prompt
)


result = agent.invoke({
    "messages": [{"role": "user", "content": "Who won the World Series in 2023?"}]
})
print(result["messages"][-1].content)